import streamlit as st
import os
import pandas as pd
from datetime import datetime

# Mock imports for type hints (won't be used if packages aren't installed)
try:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import Chroma
    from langchain_community.llms import Ollama
    from langchain_core.prompts import PromptTemplate
    from langchain_core.runnables import RunnablePassthrough
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.language_models import FakeListLLM
    PACKAGES_AVAILABLE = True
except ImportError:
    PACKAGES_AVAILABLE = False

# Page configuration
st.set_page_config(
    page_title="SEC RAG Explorer",
    page_icon="üí∏",
    layout="wide",
    initial_sidebar_state="expanded"
)

# -----------------------------------------------------------------------------
# CSS / THEME
# -----------------------------------------------------------------------------
st.markdown("""
<style>
    /* Global Glassmorphism Theme */
    .stApp {
        background: linear-gradient(135deg, #0f172a 0%, #1e293b 100%);
        color: #e2e8f0;
    }
    
    /* Sidebar */
    [data-testid="stSidebar"] {
        background-color: rgba(15, 23, 42, 0.9);
        border-right: 1px solid rgba(148, 163, 184, 0.1);
    }
    
    /* Metrics */
    .metric-card {
        background: rgba(30, 41, 59, 0.6);
        border: 1px solid rgba(148, 163, 184, 0.1);
        border-radius: 12px;
        padding: 1rem;
        backdrop-filter: blur(10px);
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        text-align: center;
    }
    .metric-value {
        font-size: 2rem;
        font-weight: 700;
        color: #38bdf8;
    }
    .metric-label {
        font-size: 0.875rem;
        color: #94a3b8;
    }
    
    /* Chat Message Bubbles */
    .chat-user {
        background: rgba(56, 189, 248, 0.1);
        border: 1px solid rgba(56, 189, 248, 0.2);
        border-radius: 12px;
        padding: 1rem;
        margin-bottom: 1rem;
    }
    .chat-bot {
        background: rgba(30, 41, 59, 0.6);
        border: 1px solid rgba(148, 163, 184, 0.1);
        border-radius: 12px;
        padding: 1rem;
        margin-bottom: 1rem;
    }
    
    /* Confidence Badges */
    .confidence-high {
        background-color: rgba(34, 197, 94, 0.2);
        color: #4ade80;
        padding: 2px 8px;
        border-radius: 12px;
        font-size: 0.75rem;
        border: 1px solid rgba(34, 197, 94, 0.3);
    }
    .confidence-med {
        background-color: rgba(234, 179, 8, 0.2);
        color: #facc15;
        padding: 2px 8px;
        border-radius: 12px;
        font-size: 0.75rem;
        border: 1px solid rgba(234, 179, 8, 0.3);
    }
    .confidence-low {
        background-color: rgba(239, 68, 68, 0.2);
        color: #f87171;
        padding: 2px 8px;
        border-radius: 12px;
        font-size: 0.75rem;
        border: 1px solid rgba(239, 68, 68, 0.3);
    }
</style>
""", unsafe_allow_html=True)

# -----------------------------------------------------------------------------
# INITIALIZATION
# -----------------------------------------------------------------------------
DB_PATH = "chroma_db"

@st.cache_resource
def get_resources():
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    if os.path.exists(DB_PATH):
        vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    else:
        return None, None
    
    try:
        llm = Ollama(model="llama3")
        llm.invoke("Test") # wake up
    except:
        try:
            llm = Ollama(model="mistral")
        except:
            # If Ollama is not available, use a mock LLM
            from langchain_core.language_models import FakeListLLM
            llm = FakeListLLM(responses=["This is a mock response. In a real implementation, this would be generated by an LLM."])
    
    return vectorstore, llm

vectorstore, llm = get_resources()

def calculate_confidence(score):
    # Chroma uses Euclidean distance (L2). 
    # 0 = exact match. 
    # > 1.0 = poor match.
    # Convert to 0-100%
    if score > 1.5:
        return 0.0
    return max(0, (1.5 - score) / 1.5) * 100

# -----------------------------------------------------------------------------
# SIDEBAR
# -----------------------------------------------------------------------------
with st.sidebar:
    st.image("https://upload.wikimedia.org/wikipedia/commons/thumb/f/f3/SEC_logo.svg/1024px-SEC_logo.svg.png", width=50)
    st.title("SEC RAG Explorer")
    st.markdown("---")
    
    st.header("‚öôÔ∏è Settings")
    model_name = st.selectbox("LLM Model", ["llama3", "mistral"], index=0)
    
    st.markdown("### Filters")
    # In a real app, populate these from vectorstore metadata
    company_filter = st.selectbox("Company", ["All", "Apple", "Microsoft", "Tesla", "Unknown"], index=0)
    year_filter = st.selectbox("Year", ["All", "2023", "2022", "2021"], index=0)
    
    st.markdown("---")
    st.info("System Ready\n\nVector DB: Chroma\nEmbeddings: MiniLM-L6")

# -----------------------------------------------------------------------------
# MAIN APP
# -----------------------------------------------------------------------------

if not vectorstore:
    st.error("üö® Vector Database not found! Please run `python ingest.py` first.")
    st.stop()

tab_dashboard, tab_chat, tab_compare = st.tabs(["üìä Dashboard", "üí¨ Chat", "‚öñÔ∏è Compare"])

# --- TAB 1: DASHBOARD ---
with tab_dashboard:
    st.header("Overview")
    
    # Get some quick stats from Chroma
    collection = vectorstore._collection
    count = collection.count()
    
    c1, c2, c3 = st.columns(3)
    with c1:
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-value">{count}</div>
            <div class="metric-label">Document Chunks</div>
        </div>
        """, unsafe_allow_html=True)
    with c2:
        st.markdown("""
        <div class="metric-card">
            <div class="metric-value">5</div>
            <div class="metric-label">Filings Ingested</div>
        </div>
        """, unsafe_allow_html=True)
    with c3:
        st.markdown("""
        <div class="metric-card">
            <div class="metric-value">0.8s</div>
            <div class="metric-label">Avg Retrieval Time</div>
        </div>
        """, unsafe_allow_html=True)

    st.markdown("### Recent Filings")
    # Mock data for UI demo (since getting distinct metadata from Chroma is slow)
    st.dataframe(pd.DataFrame({
        "Company": ["Libery Star Gold", "Geokinetics Inc", "Black Hills Corp", "Monaker Group", "OSullivan"],
        "Form": ["8-K", "8-K", "10-K", "8-K", "8-K"],
        "Date": ["2006-10-27", "2009-02-18", "2009-03-02", "2020-11-19", "2004-05-18"],
        "Section": ["Ex-10", "Ex-10", "Risk Factors", "Ex-10", "Ex-10"]
    }), use_container_width=True)

# --- TAB 2: CHAT ---
with tab_chat:
    st.header("Analysis")
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display chat
    for message in st.session_state.messages:
        role_class = "chat-user" if message["role"] == "user" else "chat-bot"
        with st.container():
            st.markdown(f"""
            <div class="{role_class}">
                <b>{message["role"].title()}:</b> {message["content"]}
            </div>
            """, unsafe_allow_html=True)
            
            if "sources" in message:
                with st.expander("üìö View Sources & Confidence"):
                    conf = message.get("confidence", 0)
                    color_class = "confidence-high" if conf > 70 else ("confidence-med" if conf > 40 else "confidence-low")
                    st.markdown(f'<span class="{color_class}">Confidence: {conf:.1f}%</span>', unsafe_allow_html=True)
                    
                    for i, doc in enumerate(message["sources"]):
                        st.markdown(f"**Source {i+1}**: {doc.metadata.get('company', 'Unknown')} ({doc.metadata.get('year', 'N/A')})")
                        st.text(doc.page_content[:300] + "...")
                        st.markdown("---")

    # Input
    if prompt := st.chat_input("Ask a question about the filings..."):
        # Add user message
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Retrieval with Score
        with st.spinner("Analyzing..."):
            results = vectorstore.similarity_search_with_score(prompt, k=4)
            
            if not results:
                response = "I couldn't find any relevant information in the documents."
                sources = []
                confidence = 0
            else:
                # Calculate aggregate confidence
                scores = [calculate_confidence(score) for doc, score in results]
                avg_confidence = sum(scores) / len(scores) if scores else 0
                
                # Context construction
                context_text = "\n\n".join([doc.page_content for doc, _ in results])
                
                # Generation
                qa_prompt = f"""You are a financial analyst assistant. Use the following context to answer the question.
                If the answer is not in the context, say "I don't know based on the provided documents".
                
                Context:
                {context_text}
                
                Question: {prompt}
                
                Answer:"""
                
                response = llm.invoke(qa_prompt)
                sources = [doc for doc, _ in results] # Keep docs
                
                # Add assistant message
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response,
                    "sources": sources,
                    "confidence": avg_confidence
                })
        
        st.rerun()

# --- TAB 3: COMPARE ---
with tab_compare:
    st.header("Comparative Analysis")
    
    col1, col2 = st.columns(2)
    with col1:
        q1 = st.text_input("Entity 1 / Topic", placeholder="e.g. Apple Risk Factors")
    with col2:
        q2 = st.text_input("Entity 2 / Topic", placeholder="e.g. Microsoft Risk Factors")
        
    compare_btn = st.button("Compare")
    
    if compare_btn and q1 and q2:
        with st.spinner("Retrieving and comparing..."):
            # Retrieve for both
            docs1 = vectorstore.similarity_search(q1, k=3)
            docs2 = vectorstore.similarity_search(q2, k=3)
            
            text1 = "\n".join([d.page_content for d in docs1])
            text2 = "\n".join([d.page_content for d in docs2])
            
            comparison_prompt = f"""Compare the following two sets of financial contexts. Highlight key differences and similarities.
            
            Set 1 ({q1}):
            {text1}
            
            Set 2 ({q2}):
            {text2}
            
            Comparison:"""
            
            comparison_result = llm.invoke(comparison_prompt)
            
            st.markdown("### Analysis")
            st.write(comparison_result)
            
            with st.expander("View Underlying Context"):
                st.subheader(f"Context 1 ({q1})")
                st.write(text1[:1000] + "...")
                st.subheader(f"Context 2 ({q2})")
                st.write(text2[:1000] + "...")